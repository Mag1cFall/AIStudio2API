"""
è¯·æ±‚å¤„ç†å™¨æ¨¡å—
åŒ…å«æ ¸å¿ƒçš„è¯·æ±‚å¤„ç†é€»è¾‘
"""

import asyncio
import json
import os
import random
import time
from typing import Optional, Tuple, Callable, AsyncGenerator
from asyncio import Event, Future

from fastapi import HTTPException, Request
from fastapi.responses import JSONResponse, StreamingResponse
from playwright.async_api import Page as AsyncPage, Locator, Error as PlaywrightAsyncError, expect as expect_async

# --- é…ç½®æ¨¡å—å¯¼å…¥ ---
from config import *

# --- modelsæ¨¡å—å¯¼å…¥ ---
from models import ChatCompletionRequest, ClientDisconnectedError

# --- browser_utilsæ¨¡å—å¯¼å…¥ ---
from browser_utils import (
    switch_ai_studio_model,
    save_error_snapshot
)

# --- api_utilsæ¨¡å—å¯¼å…¥ ---
from .utils import (
    validate_chat_request,
    prepare_combined_prompt,
    generate_sse_chunk,
    generate_sse_stop_chunk,
    use_stream_response,
    calculate_usage_stats,
    request_manager
)
from .abort_detector import AbortSignalDetector, AbortSignalHandler
from browser_utils.page_controller import PageController


async def _initialize_request_context(req_id: str, request: ChatCompletionRequest) -> dict:
    """åˆå§‹åŒ–è¯·æ±‚ä¸Šä¸‹æ–‡"""
    from server import (
        logger, page_instance, is_page_ready, parsed_model_list,
        current_ai_studio_model_id, model_switching_lock, page_params_cache,
        params_cache_lock
    )
    
    # æ³¨å†Œè¯·æ±‚åˆ°å–æ¶ˆç®¡ç†å™¨
    request_manager.register_request(req_id, {
        'model': request.model,
        'stream': request.stream,
        'message_count': len(request.messages)
    })
    
    logger.info(f"[{req_id}] å¼€å§‹å¤„ç†è¯·æ±‚...")
    logger.info(f"[{req_id}]   è¯·æ±‚å‚æ•° - Model: {request.model}, Stream: {request.stream}")
    
    context = {
        'logger': logger,
        'page': page_instance,
        'is_page_ready': is_page_ready,
        'parsed_model_list': parsed_model_list,
        'current_ai_studio_model_id': current_ai_studio_model_id,
        'model_switching_lock': model_switching_lock,
        'page_params_cache': page_params_cache,
        'params_cache_lock': params_cache_lock,
        'is_streaming': request.stream,
        'model_actually_switched': False,
        'requested_model': request.model,
        'model_id_to_use': None,
        'needs_model_switching': False
    }
    
    return context


async def _analyze_model_requirements(req_id: str, context: dict, request: ChatCompletionRequest) -> dict:
    """åˆ†ææ¨¡å‹éœ€æ±‚å¹¶ç¡®å®šæ˜¯å¦éœ€è¦åˆ‡æ¢"""
    logger = context['logger']
    current_ai_studio_model_id = context['current_ai_studio_model_id']
    parsed_model_list = context['parsed_model_list']
    requested_model = request.model
    
    if requested_model and requested_model != MODEL_NAME:
        requested_model_id = requested_model.split('/')[-1]
        logger.info(f"[{req_id}] è¯·æ±‚ä½¿ç”¨æ¨¡å‹: {requested_model_id}")
        
        if parsed_model_list:
            valid_model_ids = [m.get("id") for m in parsed_model_list]
            if requested_model_id not in valid_model_ids:
                raise HTTPException(
                    status_code=400,
                    detail=f"[{req_id}] Invalid model '{requested_model_id}'. Available models: {', '.join(valid_model_ids)}"
                )
        
        context['model_id_to_use'] = requested_model_id
        if current_ai_studio_model_id != requested_model_id:
            context['needs_model_switching'] = True
            logger.info(f"[{req_id}] éœ€è¦åˆ‡æ¢æ¨¡å‹: å½“å‰={current_ai_studio_model_id} -> ç›®æ ‡={requested_model_id}")
    
    return context


async def _test_client_connection(req_id: str, http_request: Request) -> bool:
    """å¢å¼ºçš„å®¢æˆ·ç«¯è¿æ¥æ£€æµ‹ï¼Œä¸“é—¨é’ˆå¯¹Cherry Studioå®æ—¶æ£€æµ‹ä¼˜åŒ–"""
    from server import logger
    
    try:
        # æ–¹æ³•1ï¼šåŸºç¡€æ–­å¼€æ£€æµ‹ - å¢åŠ è°ƒè¯•æ—¥å¿—
        is_disconnected = await http_request.is_disconnected()
        if is_disconnected:
            logger.info(f"[{req_id}] ğŸš¨ æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€ - is_disconnected() = True")
            return False
        
        # æ–¹æ³•2ï¼šå¢å¼ºçš„ASGIæ¶ˆæ¯æ£€æµ‹ - æé«˜æ•æ„Ÿåº¦
        if hasattr(http_request, '_receive'):
            import asyncio
            try:
                # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°50msï¼Œæé«˜æ£€æµ‹æˆåŠŸç‡
                receive_task = asyncio.create_task(http_request._receive())
                done, pending = await asyncio.wait([receive_task], timeout=0.05)  # 50msè¶…æ—¶
                
                if done:
                    message = receive_task.result()
                    message_type = message.get("type", "unknown")
                    
                    # å¢åŠ è¯¦ç»†çš„ASGIæ¶ˆæ¯æ—¥å¿—
                    logger.info(f"[{req_id}] ğŸ” æ”¶åˆ°ASGIæ¶ˆæ¯: type={message_type}, body_size={len(message.get('body', b''))}, more_body={message.get('more_body', 'N/A')}")
                    
                    # Cherry Studioåœæ­¢ä¼šå‘é€http.disconnect
                    if message_type == "http.disconnect":
                        logger.info(f"[{req_id}] ğŸš¨ Cherry Studioåœæ­¢ä¿¡å· - http.disconnect")
                        return False
                    
                    # æ£€æŸ¥å…¶ä»–æ–­å¼€ä¿¡å·
                    if message_type in ["websocket.disconnect", "websocket.close"]:
                        logger.info(f"[{req_id}] ğŸš¨ WebSocketæ–­å¼€ä¿¡å· - {message_type}")
                        return False
                        
                    # å¢å¼ºçš„ç©ºbodyæ£€æµ‹
                    if message_type == "http.request":
                        body = message.get("body", b"")
                        more_body = message.get("more_body", True)
                        
                        # Cherry Studioå¯èƒ½å‘é€ç‰¹æ®Šçš„åœæ­¢ä¿¡å·
                        if body == b"" and not more_body:
                            logger.info(f"[{req_id}] ğŸš¨ ç©ºbodyåœæ­¢ä¿¡å·")
                            return False
                        
                        # æ£€æŸ¥bodyä¸­æ˜¯å¦åŒ…å«åœæ­¢ç›¸å…³å†…å®¹
                        if body:
                            body_str = body.decode('utf-8', errors='ignore').lower()
                            if any(stop_word in body_str for stop_word in ['abort', 'cancel', 'stop']):
                                logger.info(f"[{req_id}] ğŸš¨ æ£€æµ‹åˆ°åœæ­¢å…³é”®è¯åœ¨bodyä¸­: {body_str[:100]}")
                                return False
                else:
                    # æ¸…ç†pendingä»»åŠ¡
                    for task in pending:
                        task.cancel()
                        try:
                            await task
                        except asyncio.CancelledError:
                            pass
                            
            except asyncio.TimeoutError:
                # è¶…æ—¶æ˜¯æ­£å¸¸çš„ï¼Œç»§ç»­æ£€æµ‹
                pass
            except Exception as e:
                logger.warning(f"[{req_id}] ASGIæ¶ˆæ¯æ£€æµ‹å¼‚å¸¸: {e}")
                # æ£€æµ‹å¼‚å¸¸å¯èƒ½è¡¨ç¤ºè¿æ¥é—®é¢˜
                error_msg = str(e).lower()
                if any(keyword in error_msg for keyword in ['disconnect', 'closed', 'abort', 'cancel', 'reset', 'broken']):
                    logger.info(f"[{req_id}] ğŸš¨ å¼‚å¸¸è¡¨ç¤ºæ–­å¼€è¿æ¥: {e}")
                    return False
        
        # æ–¹æ³•3ï¼šå°è¯•æ£€æŸ¥ä¼ è¾“å±‚çŠ¶æ€
        try:
            if hasattr(http_request, 'scope'):
                scope = http_request.scope
                transport = scope.get('transport')
                if transport:
                    # æ£€æŸ¥ä¼ è¾“å±‚æ˜¯å¦å…³é—­
                    if hasattr(transport, 'is_closing') and transport.is_closing():
                        logger.info(f"[{req_id}] ğŸš¨ ä¼ è¾“å±‚æ­£åœ¨å…³é—­")
                        return False
                    if hasattr(transport, 'is_closed') and transport.is_closed():
                        logger.info(f"[{req_id}] ğŸš¨ ä¼ è¾“å±‚å·²å…³é—­")
                        return False
        except Exception:
            pass
        
        return True
        
    except Exception as e:
        logger.warning(f"[{req_id}] è¿æ¥æ£€æµ‹æ€»å¼‚å¸¸: {e}")
        return False

async def _setup_disconnect_monitoring(req_id: str, http_request: Request, result_future: Future, page: AsyncPage) -> Tuple[Event, asyncio.Task, Callable]:
    """è®¾ç½®å®¢æˆ·ç«¯æ–­å¼€è¿æ¥ç›‘æ§ - å¢å¼ºè°ƒè¯•ç‰ˆæœ¬"""
    from server import logger

    client_disconnected_event = Event()
    page_controller = PageController(page, logger, req_id)
    
    logger.info(f"[{req_id}] ğŸš€ åˆ›å»ºå®¢æˆ·ç«¯æ–­å¼€ç›‘æ§ä»»åŠ¡")

    async def check_disconnect_periodically():
        consecutive_disconnect_count = 0
        loop_count = 0
        
        logger.info(f"[{req_id}] ğŸ”„ ç›‘æ§å¾ªç¯å¼€å§‹è¿è¡Œï¼Œ50msæ£€æµ‹é¢‘ç‡")
        
        while not client_disconnected_event.is_set():
            try:
                loop_count += 1
                
                # æ¯ç§’è®°å½•ä¸€æ¬¡ç›‘æ§çŠ¶æ€ (20æ¬¡å¾ªç¯ = 1ç§’)
                if loop_count % 20 == 0:
                    logger.info(f"[{req_id}] ğŸ’¡ ç›‘æ§è¿›è¡Œä¸­... å·²æ£€æŸ¥{loop_count}æ¬¡ ({loop_count * 0.05:.1f}ç§’)")
                
                # ä¸»åŠ¨è¿æ¥æ£€æµ‹
                is_connected = await _test_client_connection(req_id, http_request)
                
                if not is_connected:
                    consecutive_disconnect_count += 1
                    logger.info(f"[{req_id}] ğŸš¨ ä¸»åŠ¨æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€ï¼(ç¬¬{consecutive_disconnect_count}æ¬¡)")
                    
                    client_disconnected_event.set()
                    if not result_future.done():
                        result_future.set_exception(HTTPException(status_code=499, detail=f"[{req_id}] å®¢æˆ·ç«¯å…³é—­äº†è¯·æ±‚"))
                    
                    # è°ƒç”¨é¡µé¢åœæ­¢ç”Ÿæˆ - ä¿®å¤å‚æ•°ä¼ é€’
                    logger.info(f"[{req_id}] ğŸ›‘ å®¢æˆ·ç«¯æ–­å¼€ï¼Œæ­£åœ¨è°ƒç”¨é¡µé¢åœæ­¢ç”Ÿæˆ...")
                    try:
                        # åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„æ–­å¼€æ£€æµ‹å‡½æ•°ï¼Œå› ä¸ºå®¢æˆ·ç«¯å·²ç»æ–­å¼€äº†
                        def simple_disconnect_check(stage=""):
                            return False  # ä¸éœ€è¦å†æ£€æµ‹ï¼Œç›´æ¥æ‰§è¡Œåœæ­¢
                        
                        await page_controller.stop_generation(simple_disconnect_check)
                        logger.info(f"[{req_id}] âœ… é¡µé¢åœæ­¢ç”Ÿæˆå‘½ä»¤æ‰§è¡ŒæˆåŠŸ")
                    except Exception as stop_err:
                        logger.error(f"[{req_id}] âŒ é¡µé¢åœæ­¢ç”Ÿæˆå¤±è´¥: {stop_err}")
                    break
                else:
                    consecutive_disconnect_count = 0

                # å¤‡ç”¨æ£€æµ‹
                backup_disconnected = await http_request.is_disconnected()
                if backup_disconnected:
                    logger.info(f"[{req_id}] ğŸš¨ å¤‡ç”¨æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€è¿æ¥")
                    client_disconnected_event.set()
                    if not result_future.done():
                        result_future.set_exception(HTTPException(status_code=499, detail=f"[{req_id}] å®¢æˆ·ç«¯å…³é—­äº†è¯·æ±‚"))
                    
                    logger.info(f"[{req_id}] ğŸ›‘ å®¢æˆ·ç«¯æ–­å¼€ï¼ˆå¤‡ç”¨æ£€æµ‹ï¼‰ï¼Œæ­£åœ¨è°ƒç”¨é¡µé¢åœæ­¢ç”Ÿæˆ...")
                    try:
                        # åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„æ–­å¼€æ£€æµ‹å‡½æ•°ï¼Œå› ä¸ºå®¢æˆ·ç«¯å·²ç»æ–­å¼€äº†
                        def simple_disconnect_check(stage=""):
                            return False  # ä¸éœ€è¦å†æ£€æµ‹ï¼Œç›´æ¥æ‰§è¡Œåœæ­¢
                        
                        await page_controller.stop_generation(simple_disconnect_check)
                        logger.info(f"[{req_id}] âœ… å¤‡ç”¨æ£€æµ‹é¡µé¢åœæ­¢ç”Ÿæˆå‘½ä»¤æ‰§è¡ŒæˆåŠŸ")
                    except Exception as stop_err:
                        logger.error(f"[{req_id}] âŒ å¤‡ç”¨æ£€æµ‹é¡µé¢åœæ­¢ç”Ÿæˆå¤±è´¥: {stop_err}")
                    break

                await asyncio.sleep(0.05)  # 50msæ£€æµ‹é¢‘ç‡
                
            except asyncio.CancelledError:
                logger.info(f"[{req_id}] ğŸ“› ç›‘æ§ä»»åŠ¡è¢«å–æ¶ˆ")
                break
            except Exception as e:
                logger.error(f"[{req_id}] âŒ ç›‘æ§å¾ªç¯å¼‚å¸¸: {e}")
                client_disconnected_event.set()
                if not result_future.done():
                    result_future.set_exception(HTTPException(status_code=500, detail=f"[{req_id}] Internal disconnect checker error: {e}"))
                break
        
        logger.info(f"[{req_id}] ğŸ ç›‘æ§å¾ªç¯ç»“æŸï¼Œæ€»å…±è¿è¡Œäº†{loop_count}æ¬¡å¾ªç¯")

    disconnect_check_task = asyncio.create_task(check_disconnect_periodically())
    logger.info(f"[{req_id}] âœ… ç›‘æ§ä»»åŠ¡å·²åˆ›å»ºå¹¶å¯åŠ¨: {disconnect_check_task}")

    def check_client_disconnected(stage: str = ""):
        if request_manager.is_cancelled(req_id):
            logger.info(f"[{req_id}] åœ¨ '{stage}' æ£€æµ‹åˆ°è¯·æ±‚è¢«ç”¨æˆ·å–æ¶ˆã€‚")
            raise ClientDisconnectedError(f"[{req_id}] Request cancelled by user at stage: {stage}")
        
        if client_disconnected_event.is_set():
            logger.info(f"[{req_id}] åœ¨ '{stage}' æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€è¿æ¥ã€‚")
            raise ClientDisconnectedError(f"[{req_id}] Client disconnected at stage: {stage}")
        return False

    return client_disconnected_event, disconnect_check_task, check_client_disconnected


async def _validate_page_status(req_id: str, context: dict, check_client_disconnected: Callable) -> None:
    """éªŒè¯é¡µé¢çŠ¶æ€"""
    page = context['page']
    is_page_ready = context['is_page_ready']
    
    if not page or page.is_closed() or not is_page_ready:
        raise HTTPException(status_code=503, detail=f"[{req_id}] AI Studio é¡µé¢ä¸¢å¤±æˆ–æœªå°±ç»ªã€‚", headers={"Retry-After": "30"})
    
    check_client_disconnected("Initial Page Check")


async def _handle_model_switching(req_id: str, context: dict, check_client_disconnected: Callable) -> dict:
    """å¤„ç†æ¨¡å‹åˆ‡æ¢é€»è¾‘"""
    if not context['needs_model_switching']:
        return context
    
    logger = context['logger']
    page = context['page']
    model_switching_lock = context['model_switching_lock']
    model_id_to_use = context['model_id_to_use']
    
    import server
    
    async with model_switching_lock:
        if server.current_ai_studio_model_id != model_id_to_use:
            logger.info(f"[{req_id}] å‡†å¤‡åˆ‡æ¢æ¨¡å‹: {server.current_ai_studio_model_id} -> {model_id_to_use}")
            switch_success = await switch_ai_studio_model(page, model_id_to_use, req_id)
            if switch_success:
                server.current_ai_studio_model_id = model_id_to_use
                context['model_actually_switched'] = True
                context['current_ai_studio_model_id'] = model_id_to_use
                logger.info(f"[{req_id}]  æ¨¡å‹åˆ‡æ¢æˆåŠŸ: {server.current_ai_studio_model_id}")
            else:
                await _handle_model_switch_failure(req_id, page, model_id_to_use, server.current_ai_studio_model_id, logger)
    
    return context


async def _handle_model_switch_failure(req_id: str, page: AsyncPage, model_id_to_use: str, model_before_switch: str, logger) -> None:
    """å¤„ç†æ¨¡å‹åˆ‡æ¢å¤±è´¥çš„æƒ…å†µ"""
    import server
    
    logger.warning(f"[{req_id}] âŒ æ¨¡å‹åˆ‡æ¢è‡³ {model_id_to_use} å¤±è´¥ã€‚")
    # å°è¯•æ¢å¤å…¨å±€çŠ¶æ€
    server.current_ai_studio_model_id = model_before_switch
    
    raise HTTPException(
        status_code=422,
        detail=f"[{req_id}] æœªèƒ½åˆ‡æ¢åˆ°æ¨¡å‹ '{model_id_to_use}'ã€‚è¯·ç¡®ä¿æ¨¡å‹å¯ç”¨ã€‚"
    )


async def _handle_parameter_cache(req_id: str, context: dict) -> None:
    """å¤„ç†å‚æ•°ç¼“å­˜"""
    logger = context['logger']
    params_cache_lock = context['params_cache_lock']
    page_params_cache = context['page_params_cache']
    current_ai_studio_model_id = context['current_ai_studio_model_id']
    model_actually_switched = context['model_actually_switched']
    
    async with params_cache_lock:
        cached_model_for_params = page_params_cache.get("last_known_model_id_for_params")
        
        if model_actually_switched or (current_ai_studio_model_id != cached_model_for_params):
            logger.info(f"[{req_id}] æ¨¡å‹å·²æ›´æ”¹ï¼Œå‚æ•°ç¼“å­˜å¤±æ•ˆã€‚")
            page_params_cache.clear()
            page_params_cache["last_known_model_id_for_params"] = current_ai_studio_model_id


async def _prepare_and_validate_request(req_id: str, request: ChatCompletionRequest, check_client_disconnected: Callable) -> Tuple[str, str, list]:
    """å‡†å¤‡å’ŒéªŒè¯è¯·æ±‚"""
    from server import logger
    
    try:
        validate_chat_request(request.messages, req_id)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=f"[{req_id}] æ— æ•ˆè¯·æ±‚: {e}")
    
    # ç›´æ¥å°†æ¶ˆæ¯ä¼ é€’ç»™ prepare_combined_prompt è¿›è¡Œå¤„ç†
    # å®ƒä¼šåŒæ—¶è¿”å›æ ¼å¼åŒ–çš„æç¤ºå’Œéœ€è¦ä¸Šä¼ çš„å›¾ç‰‡åˆ—è¡¨
    system_prompt, prepared_prompt, final_image_list = prepare_combined_prompt(request.messages, req_id)
    check_client_disconnected("After Prompt Prep")

    # ç¡®ä¿å›¾ç‰‡åˆ—è¡¨ä¸ä¸ºç©ºæ—¶è®°å½•æ—¥å¿—
    if final_image_list:
        logger.info(f"[{req_id}] å‡†å¤‡ä¸Šä¼  {len(final_image_list)} å¼ å›¾ç‰‡åˆ°é¡µé¢")
    else:
        logger.info(f"[{req_id}] æ²¡æœ‰æ£€æµ‹åˆ°éœ€è¦ä¸Šä¼ çš„å›¾ç‰‡")
    
    return system_prompt, prepared_prompt, final_image_list

async def _handle_response_processing(req_id: str, request: ChatCompletionRequest, page: AsyncPage,
                                    context: dict, result_future: Future,
                                    submit_button_locator: Locator, check_client_disconnected: Callable, disconnect_check_task: Optional[asyncio.Task]) -> Optional[Tuple[Event, Locator, Callable]]:
    """å¤„ç†å“åº”ç”Ÿæˆ"""
    from server import logger
    
    is_streaming = request.stream
    current_ai_studio_model_id = context.get('current_ai_studio_model_id')
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨è¾…åŠ©æµ
    stream_port = os.environ.get('STREAM_PORT')
    use_stream = stream_port != '0'
    
    if use_stream:
        return await _handle_auxiliary_stream_response(req_id, request, context, result_future, submit_button_locator, check_client_disconnected, disconnect_check_task)
    else:
        return await _handle_playwright_response(req_id, request, page, context, result_future, submit_button_locator, check_client_disconnected)


async def _handle_auxiliary_stream_response(req_id: str, request: ChatCompletionRequest, context: dict, 
                                          result_future: Future, submit_button_locator: Locator, 
                                          check_client_disconnected: Callable, disconnect_check_task: Optional[asyncio.Task]) -> Optional[Tuple[Event, Locator, Callable]]:
    """ä½¿ç”¨è¾…åŠ©æµå¤„ç†å“åº”"""
    from server import logger
    
    is_streaming = request.stream
    current_ai_studio_model_id = context.get('current_ai_studio_model_id')
    
    def generate_random_string(length):
        charset = "abcdefghijklmnopqrstuvwxyz0123456789"
        return ''.join(random.choice(charset) for _ in range(length))

    if is_streaming:
        try:
            completion_event = Event()
            
            async def create_stream_generator_from_helper(event_to_set: Event, task_to_cancel: Optional[asyncio.Task]) -> AsyncGenerator[str, None]:
                last_reason_pos = 0
                last_body_pos = 0
                model_name_for_stream = current_ai_studio_model_id or MODEL_NAME
                chat_completion_id = f"{CHAT_COMPLETION_ID_PREFIX}{req_id}-{int(time.time())}-{random.randint(100, 999)}"
                created_timestamp = int(time.time())

                # ç”¨äºæ”¶é›†å®Œæ•´å†…å®¹ä»¥è®¡ç®—usage
                full_reasoning_content = ""
                full_body_content = ""

                # æ•°æ®æ¥æ”¶çŠ¶æ€æ ‡è®°
                data_receiving = False

                try:
                    async for raw_data in use_stream_response(req_id):
                        # æ ‡è®°æ•°æ®æ¥æ”¶çŠ¶æ€
                        data_receiving = True

                        # åŒé‡æ£€æŸ¥å®¢æˆ·ç«¯è¿æ¥çŠ¶æ€ - æ—¢æ£€æŸ¥äº‹ä»¶ä¹Ÿç›´æ¥æ£€æµ‹è¿æ¥
                        try:
                            check_client_disconnected(f"æµå¼ç”Ÿæˆå™¨å¾ªç¯ ({req_id}): ")
                        except ClientDisconnectedError:
                            logger.info(f"[{req_id}] ğŸš¨ æµå¼ç”Ÿæˆå™¨æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€è¿æ¥ï¼ˆé€šè¿‡äº‹ä»¶ï¼‰")
                            # å¦‚æœæ­£åœ¨æ¥æ”¶æ•°æ®æ—¶å®¢æˆ·ç«¯æ–­å¼€ï¼Œç«‹å³è®¾ç½®doneä¿¡å·
                            if data_receiving and not event_to_set.is_set():
                                logger.info(f"[{req_id}] æ•°æ®æ¥æ”¶ä¸­å®¢æˆ·ç«¯æ–­å¼€ï¼Œç«‹å³è®¾ç½®doneä¿¡å·")
                                event_to_set.set()
                            # å‘é€åœæ­¢å—å¹¶é€€å‡º
                            try:
                                stop_chunk = {
                                    "id": chat_completion_id,
                                    "object": "chat.completion.chunk",
                                    "model": model_name_for_stream,
                                    "created": created_timestamp,
                                    "choices": [{
                                        "index": 0,
                                        "delta": {"role": "assistant"},
                                        "finish_reason": "stop",
                                        "native_finish_reason": "stop",
                                    }]
                                }
                                yield f"data: {json.dumps(stop_chunk, ensure_ascii=False, separators=(',', ':'))}\n\n"
                                yield "data: [DONE]\n\n"
                            except Exception:
                                pass  # å¿½ç•¥å‘é€åœæ­¢å—æ—¶çš„é”™è¯¯
                            break
                        
                        # è¡¥å……ç‹¬ç«‹çš„è¿æ¥æ£€æµ‹ - é˜²æ­¢å¤–éƒ¨ç›‘æ§ä»»åŠ¡å¤±æ•ˆ
                        try:
                            # è·å–HTTPè¯·æ±‚å¯¹è±¡è¿›è¡Œç›´æ¥æ£€æµ‹
                            import server
                            # ä»å…¨å±€çŠ¶æ€è·å–å½“å‰è¯·æ±‚çš„HTTPå¯¹è±¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                            if hasattr(server, 'current_http_requests') and req_id in server.current_http_requests:
                                current_http_request = server.current_http_requests[req_id]
                                is_connected = await _test_client_connection(req_id, current_http_request)
                                if not is_connected:
                                    logger.info(f"[{req_id}] ğŸš¨ æµå¼ç”Ÿæˆå™¨ç‹¬ç«‹æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€ï¼")
                                    if data_receiving and not event_to_set.is_set():
                                        event_to_set.set()
                                    try:
                                        stop_chunk = {
                                            "id": chat_completion_id,
                                            "object": "chat.completion.chunk",
                                            "model": model_name_for_stream,
                                            "created": created_timestamp,
                                            "choices": [{
                                                "index": 0,
                                                "delta": {"role": "assistant"},
                                                "finish_reason": "stop",
                                                "native_finish_reason": "stop",
                                            }]
                                        }
                                        yield f"data: {json.dumps(stop_chunk, ensure_ascii=False, separators=(',', ':'))}\n\n"
                                        yield "data: [DONE]\n\n"
                                    except Exception:
                                        pass
                                    break
                        except Exception as direct_check_err:
                            # ç›´æ¥æ£€æµ‹å¤±è´¥ä¸å½±å“æ­£å¸¸æµç¨‹
                            pass
                        
                        # ç¡®ä¿ data æ˜¯å­—å…¸ç±»å‹
                        if isinstance(raw_data, str):
                            try:
                                data = json.loads(raw_data)
                            except json.JSONDecodeError:
                                logger.warning(f"[{req_id}] æ— æ³•è§£ææµæ•°æ®JSON: {raw_data}")
                                continue
                        elif isinstance(raw_data, dict):
                            data = raw_data
                        else:
                            logger.warning(f"[{req_id}] æœªçŸ¥çš„æµæ•°æ®ç±»å‹: {type(raw_data)}")
                            continue
                        
                        # ç¡®ä¿å¿…è¦çš„é”®å­˜åœ¨
                        if not isinstance(data, dict):
                            logger.warning(f"[{req_id}] æ•°æ®ä¸æ˜¯å­—å…¸ç±»å‹: {data}")
                            continue
                        
                        reason = data.get("reason", "")
                        body = data.get("body", "")
                        done = data.get("done", False)
                        function = data.get("function", [])
                        
                        # æ›´æ–°å®Œæ•´å†…å®¹è®°å½•
                        if reason:
                            full_reasoning_content = reason
                        if body:
                            full_body_content = body
                        
                        # å¤„ç†æ¨ç†å†…å®¹
                        if len(reason) > last_reason_pos:
                            output = {
                                "id": chat_completion_id,
                                "object": "chat.completion.chunk",
                                "model": model_name_for_stream,
                                "created": created_timestamp,
                                "choices":[{
                                    "index": 0,
                                    "delta":{
                                        "role": "assistant",
                                        "content": None,
                                        "reasoning_content": reason[last_reason_pos:],
                                    },
                                    "finish_reason": None,
                                    "native_finish_reason": None,
                                }]
                            }
                            last_reason_pos = len(reason)
                            yield f"data: {json.dumps(output, ensure_ascii=False, separators=(',', ':'))}\n\n"
                        
                        # å¤„ç†ä¸»ä½“å†…å®¹
                        if len(body) > last_body_pos:
                            finish_reason_val = None
                            if done:
                                finish_reason_val = "stop"
                            
                            delta_content = {"role": "assistant", "content": body[last_body_pos:]}
                            choice_item = {
                                "index": 0,
                                "delta": delta_content,
                                "finish_reason": finish_reason_val,
                                "native_finish_reason": finish_reason_val,
                            }

                            if done and function and len(function) > 0:
                                tool_calls_list = []
                                for func_idx, function_call_data in enumerate(function):
                                    tool_calls_list.append({
                                        "id": f"call_{generate_random_string(24)}",
                                        "index": func_idx,
                                        "type": "function",
                                        "function": {
                                            "name": function_call_data["name"],
                                            "arguments": json.dumps(function_call_data["params"]),
                                        },
                                    })
                                delta_content["tool_calls"] = tool_calls_list
                                choice_item["finish_reason"] = "tool_calls"
                                choice_item["native_finish_reason"] = "tool_calls"
                                delta_content["content"] = None

                            output = {
                                "id": chat_completion_id,
                                "object": "chat.completion.chunk",
                                "model": model_name_for_stream,
                                "created": created_timestamp,
                                "choices": [choice_item]
                            }
                            last_body_pos = len(body)
                            yield f"data: {json.dumps(output, ensure_ascii=False, separators=(',', ':'))}\n\n"
                        
                        # å¤„ç†åªæœ‰done=Trueä½†æ²¡æœ‰æ–°å†…å®¹çš„æƒ…å†µï¼ˆä»…æœ‰å‡½æ•°è°ƒç”¨æˆ–çº¯ç»“æŸï¼‰
                        elif done:
                            # å¦‚æœæœ‰å‡½æ•°è°ƒç”¨ä½†æ²¡æœ‰æ–°çš„bodyå†…å®¹
                            if function and len(function) > 0:
                                delta_content = {"role": "assistant", "content": None}
                                tool_calls_list = []
                                for func_idx, function_call_data in enumerate(function):
                                    tool_calls_list.append({
                                        "id": f"call_{generate_random_string(24)}",
                                        "index": func_idx,
                                        "type": "function",
                                        "function": {
                                            "name": function_call_data["name"],
                                            "arguments": json.dumps(function_call_data["params"]),
                                        },
                                    })
                                delta_content["tool_calls"] = tool_calls_list
                                choice_item = {
                                    "index": 0,
                                    "delta": delta_content,
                                    "finish_reason": "tool_calls",
                                    "native_finish_reason": "tool_calls",
                                }
                            else:
                                # çº¯ç»“æŸï¼Œæ²¡æœ‰æ–°å†…å®¹å’Œå‡½æ•°è°ƒç”¨
                                choice_item = {
                                    "index": 0,
                                    "delta": {"role": "assistant"},
                                    "finish_reason": "stop",
                                    "native_finish_reason": "stop",
                                }

                            output = {
                                "id": chat_completion_id,
                                "object": "chat.completion.chunk",
                                "model": model_name_for_stream,
                                "created": created_timestamp,
                                "choices": [choice_item]
                            }
                            yield f"data: {json.dumps(output, ensure_ascii=False, separators=(',', ':'))}\n\n"
                
                except ClientDisconnectedError as disconnect_err:
                    # ä½¿ç”¨æ–°çš„åœæ­¢ä¿¡å·æ£€æµ‹å™¨åˆ†ææ–­å¼€åŸå› 
                    abort_handler = AbortSignalHandler()
                    disconnect_info = abort_handler.handle_error(disconnect_err, req_id)
                    
                    logger.info(f"[{req_id}] æµå¼ç”Ÿæˆå™¨ä¸­æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€è¿æ¥")
                    logger.info(f"[{req_id}] åœæ­¢åŸå› åˆ†æ: {disconnect_info}")
                    
                    # å®¢æˆ·ç«¯æ–­å¼€æ—¶ç«‹å³è®¾ç½®doneä¿¡å·
                    if data_receiving and not event_to_set.is_set():
                        logger.info(f"[{req_id}] å®¢æˆ·ç«¯æ–­å¼€å¼‚å¸¸å¤„ç†ä¸­ç«‹å³è®¾ç½®doneä¿¡å·")
                        event_to_set.set()
                except Exception as e:
                    # ä½¿ç”¨æ–°çš„åœæ­¢ä¿¡å·æ£€æµ‹å™¨åˆ†æé”™è¯¯ç±»å‹
                    abort_handler = AbortSignalHandler()
                    error_info = abort_handler.handle_error(e, req_id)
                    
                    if error_info['stop_reason'] in ['user_abort', 'client_disconnect']:
                        logger.info(f"[{req_id}] æ£€æµ‹åˆ°åœæ­¢ä¿¡å·: {error_info}")
                        # å¯¹äºç”¨æˆ·ä¸»åŠ¨åœæ­¢ï¼Œè§†ä¸ºæ­£å¸¸æš‚åœ
                        if data_receiving and not event_to_set.is_set():
                            event_to_set.set()
                    else:
                        logger.error(f"[{req_id}] æµå¼ç”Ÿæˆå™¨å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
                    # å‘é€é”™è¯¯ä¿¡æ¯ç»™å®¢æˆ·ç«¯
                    try:
                        error_chunk = {
                            "id": chat_completion_id,
                            "object": "chat.completion.chunk",
                            "model": model_name_for_stream,
                            "created": created_timestamp,
                            "choices": [{
                                "index": 0,
                                "delta": {"role": "assistant", "content": f"\n\n[é”™è¯¯: {str(e)}]"},
                                "finish_reason": "stop",
                                "native_finish_reason": "stop",
                            }]
                        }
                        yield f"data: {json.dumps(error_chunk, ensure_ascii=False, separators=(',', ':'))}\n\n"
                    except Exception:
                        pass  # å¦‚æœæ— æ³•å‘é€é”™è¯¯ä¿¡æ¯ï¼Œç»§ç»­å¤„ç†ç»“æŸé€»è¾‘
                finally:
                    # è®¡ç®—usageç»Ÿè®¡
                    try:
                        usage_stats = calculate_usage_stats(
                            [msg.model_dump() for msg in request.messages],
                            full_body_content,
                            full_reasoning_content
                        )
                        logger.info(f"[{req_id}] è®¡ç®—çš„tokenä½¿ç”¨ç»Ÿè®¡: {usage_stats}")
                        
                        # å‘é€å¸¦usageçš„æœ€ç»ˆchunk
                        final_chunk = {
                            "id": chat_completion_id,
                            "object": "chat.completion.chunk",
                            "model": model_name_for_stream,
                            "created": created_timestamp,
                            "choices": [{
                                "index": 0,
                                "delta": {},
                                "finish_reason": "stop",
                                "native_finish_reason": "stop"
                            }],
                            "usage": usage_stats
                        }
                        yield f"data: {json.dumps(final_chunk, ensure_ascii=False, separators=(',', ':'))}\n\n"
                        logger.info(f"[{req_id}] å·²å‘é€å¸¦usageç»Ÿè®¡çš„æœ€ç»ˆchunk")
                        
                    except Exception as usage_err:
                        logger.error(f"[{req_id}] è®¡ç®—æˆ–å‘é€usageç»Ÿè®¡æ—¶å‡ºé”™: {usage_err}")
                    
                    # ç¡®ä¿æ€»æ˜¯å‘é€ [DONE] æ ‡è®°
                    try:
                        logger.info(f"[{req_id}] æµå¼ç”Ÿæˆå™¨å®Œæˆï¼Œå‘é€ [DONE] æ ‡è®°")
                        yield "data: [DONE]\n\n"
                    except Exception as done_err:
                        logger.error(f"[{req_id}] å‘é€ [DONE] æ ‡è®°æ—¶å‡ºé”™: {done_err}")
                    
                    # ç¡®ä¿äº‹ä»¶è¢«è®¾ç½®
                    if not event_to_set.is_set():
                        event_to_set.set()
                        logger.info(f"[{req_id}] æµå¼ç”Ÿæˆå™¨å®Œæˆäº‹ä»¶å·²è®¾ç½®")

                    # --- å…³é”®ä¿®å¤ï¼šåœ¨æ­¤å¤„æ¸…ç†èµ„æº ---
                    logger.info(f"[{req_id}] æµå¼ç”Ÿæˆå™¨ç»“æŸï¼Œå¼€å§‹æ¸…ç†èµ„æº...")
                    import server
                    # 1. æ¸…ç†å…¨å±€HTTPè¯·æ±‚çŠ¶æ€
                    if hasattr(server, 'current_http_requests'):
                        server.current_http_requests.pop(req_id, None)
                        logger.info(f"[{req_id}] âœ… å·²æ¸…ç†å…¨å±€HTTPè¯·æ±‚çŠ¶æ€")
                    
                    # 2. å–æ¶ˆç›‘æ§ä»»åŠ¡
                    if task_to_cancel and not task_to_cancel.done():
                        task_to_cancel.cancel()
                        logger.info(f"[{req_id}] âœ… å·²å‘é€å–æ¶ˆä¿¡å·åˆ°ç›‘æ§ä»»åŠ¡")
                    else:
                        logger.info(f"[{req_id}] âœ… ç›‘æ§ä»»åŠ¡æ— éœ€å–æ¶ˆï¼ˆå¯èƒ½å·²å®Œæˆæˆ–ä¸å­˜åœ¨ï¼‰")

            stream_gen_func = create_stream_generator_from_helper(completion_event, disconnect_check_task)
            if not result_future.done():
                result_future.set_result(StreamingResponse(stream_gen_func, media_type="text/event-stream"))
            else:
                if not completion_event.is_set():
                    completion_event.set()
            
            return completion_event, submit_button_locator, check_client_disconnected

        except Exception as e:
            logger.error(f"[{req_id}] ä»é˜Ÿåˆ—è·å–æµå¼æ•°æ®æ—¶å‡ºé”™: {e}", exc_info=True)
            if completion_event and not completion_event.is_set():
                completion_event.set()
            raise

    else:  # éæµå¼
        content = None
        reasoning_content = None
        functions = None
        final_data_from_aux_stream = None

        async for raw_data in use_stream_response(req_id):
            check_client_disconnected(f"éæµå¼è¾…åŠ©æµ - å¾ªç¯ä¸­ ({req_id}): ")
            
            # ç¡®ä¿ data æ˜¯å­—å…¸ç±»å‹
            if isinstance(raw_data, str):
                try:
                    data = json.loads(raw_data)
                except json.JSONDecodeError:
                    logger.warning(f"[{req_id}] æ— æ³•è§£æéæµå¼æ•°æ®JSON: {raw_data}")
                    continue
            elif isinstance(raw_data, dict):
                data = raw_data
            else:
                logger.warning(f"[{req_id}] éæµå¼æœªçŸ¥æ•°æ®ç±»å‹: {type(raw_data)}")
                continue
            
            # ç¡®ä¿æ•°æ®æ˜¯å­—å…¸ç±»å‹
            if not isinstance(data, dict):
                logger.warning(f"[{req_id}] éæµå¼æ•°æ®ä¸æ˜¯å­—å…¸ç±»å‹: {data}")
                continue
                
            final_data_from_aux_stream = data
            if data.get("done"):
                content = data.get("body")
                reasoning_content = data.get("reason")
                functions = data.get("function")
                break
        
        if final_data_from_aux_stream and final_data_from_aux_stream.get("reason") == "internal_timeout":
            logger.error(f"[{req_id}] éæµå¼è¯·æ±‚é€šè¿‡è¾…åŠ©æµå¤±è´¥: å†…éƒ¨è¶…æ—¶")
            raise HTTPException(status_code=502, detail=f"[{req_id}] è¾…åŠ©æµå¤„ç†é”™è¯¯ (å†…éƒ¨è¶…æ—¶)")

        if final_data_from_aux_stream and final_data_from_aux_stream.get("done") is True and content is None:
             logger.error(f"[{req_id}] éæµå¼è¯·æ±‚é€šè¿‡è¾…åŠ©æµå®Œæˆä½†æœªæä¾›å†…å®¹")
             raise HTTPException(status_code=502, detail=f"[{req_id}] è¾…åŠ©æµå®Œæˆä½†æœªæä¾›å†…å®¹")

        model_name_for_json = current_ai_studio_model_id or MODEL_NAME
        message_payload = {"role": "assistant", "content": content}
        finish_reason_val = "stop"

        if functions and len(functions) > 0:
            tool_calls_list = []
            for func_idx, function_call_data in enumerate(functions):
                tool_calls_list.append({
                    "id": f"call_{generate_random_string(24)}",
                    "index": func_idx,
                    "type": "function",
                    "function": {
                        "name": function_call_data["name"],
                        "arguments": json.dumps(function_call_data["params"]),
                    },
                })
            message_payload["tool_calls"] = tool_calls_list
            finish_reason_val = "tool_calls"
            message_payload["content"] = None
        
        if reasoning_content:
            message_payload["reasoning_content"] = reasoning_content

        # è®¡ç®—tokenä½¿ç”¨ç»Ÿè®¡
        usage_stats = calculate_usage_stats(
            [msg.model_dump() for msg in request.messages],
            content or "",
            reasoning_content
        )

        response_payload = {
            "id": f"{CHAT_COMPLETION_ID_PREFIX}{req_id}-{int(time.time())}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model_name_for_json,
            "choices": [{
                "index": 0,
                "message": message_payload,
                "finish_reason": finish_reason_val,
                "native_finish_reason": finish_reason_val,
            }],
            "usage": usage_stats
        }

        if not result_future.done():
            result_future.set_result(JSONResponse(content=response_payload))
        return None


async def _handle_playwright_response(req_id: str, request: ChatCompletionRequest, page: AsyncPage, 
                                    context: dict, result_future: Future, submit_button_locator: Locator, 
                                    check_client_disconnected: Callable) -> Optional[Tuple[Event, Locator, Callable]]:
    """ä½¿ç”¨Playwrightå¤„ç†å“åº”"""
    from server import logger
    
    is_streaming = request.stream
    current_ai_studio_model_id = context.get('current_ai_studio_model_id')
    
    logger.info(f"[{req_id}] å®šä½å“åº”å…ƒç´ ...")
    response_container = page.locator(RESPONSE_CONTAINER_SELECTOR).last
    response_element = response_container.locator(RESPONSE_TEXT_SELECTOR)
    
    try:
        await expect_async(response_container).to_be_attached(timeout=20000)
        check_client_disconnected("After Response Container Attached: ")
        await expect_async(response_element).to_be_attached(timeout=90000)
        logger.info(f"[{req_id}] å“åº”å…ƒç´ å·²å®šä½ã€‚")
    except (PlaywrightAsyncError, asyncio.TimeoutError, ClientDisconnectedError) as locate_err:
        if isinstance(locate_err, ClientDisconnectedError):
            raise
        logger.error(f"[{req_id}] âŒ é”™è¯¯: å®šä½å“åº”å…ƒç´ å¤±è´¥æˆ–è¶…æ—¶: {locate_err}")
        await save_error_snapshot(f"response_locate_error_{req_id}")
        raise HTTPException(status_code=502, detail=f"[{req_id}] å®šä½AI Studioå“åº”å…ƒç´ å¤±è´¥: {locate_err}")
    except Exception as locate_exc:
        logger.exception(f"[{req_id}] âŒ é”™è¯¯: å®šä½å“åº”å…ƒç´ æ—¶æ„å¤–é”™è¯¯")
        await save_error_snapshot(f"response_locate_unexpected_{req_id}")
        raise HTTPException(status_code=500, detail=f"[{req_id}] å®šä½å“åº”å…ƒç´ æ—¶æ„å¤–é”™è¯¯: {locate_exc}")

    check_client_disconnected("After Response Element Located: ")

    if is_streaming:
        completion_event = Event()

        async def create_response_stream_generator():
            # æ•°æ®æ¥æ”¶çŠ¶æ€æ ‡è®°
            data_receiving = False

            try:
                # ä½¿ç”¨PageControllerè·å–å“åº”
                page_controller = PageController(page, logger, req_id)
                final_content = await page_controller.get_response(check_client_disconnected)

                # æ ‡è®°æ•°æ®æ¥æ”¶çŠ¶æ€
                data_receiving = True

                # ç”Ÿæˆæµå¼å“åº” - ä¿æŒMarkdownæ ¼å¼
                # æŒ‰è¡Œåˆ†å‰²ä»¥ä¿æŒæ¢è¡Œç¬¦å’ŒMarkdownç»“æ„
                lines = final_content.split('\n')
                for line_idx, line in enumerate(lines):
                    # æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦æ–­å¼€è¿æ¥ - åœ¨æ¯ä¸ªè¾“å‡ºå—å‰éƒ½æ£€æŸ¥
                    try:
                        check_client_disconnected(f"Playwrightæµå¼ç”Ÿæˆå™¨å¾ªç¯ ({req_id}): ")
                    except ClientDisconnectedError:
                        logger.info(f"[{req_id}] Playwrightæµå¼ç”Ÿæˆå™¨ä¸­æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€è¿æ¥")
                        # å¦‚æœæ­£åœ¨æ¥æ”¶æ•°æ®æ—¶å®¢æˆ·ç«¯æ–­å¼€ï¼Œç«‹å³è®¾ç½®doneä¿¡å·
                        if data_receiving and not completion_event.is_set():
                            logger.info(f"[{req_id}] Playwrightæ•°æ®æ¥æ”¶ä¸­å®¢æˆ·ç«¯æ–­å¼€ï¼Œç«‹å³è®¾ç½®doneä¿¡å·")
                            completion_event.set()
                        # å‘é€åœæ­¢å—å¹¶é€€å‡º
                        try:
                            yield generate_sse_stop_chunk(req_id, current_ai_studio_model_id or MODEL_NAME, "stop")
                        except Exception:
                            pass  # å¿½ç•¥å‘é€åœæ­¢å—æ—¶çš„é”™è¯¯
                        break

                    # è¾“å‡ºå½“å‰è¡Œçš„å†…å®¹ï¼ˆåŒ…æ‹¬ç©ºè¡Œï¼Œä»¥ä¿æŒMarkdownæ ¼å¼ï¼‰
                    if line:  # éç©ºè¡ŒæŒ‰å­—ç¬¦åˆ†å—è¾“å‡º
                        chunk_size = 5  # æ¯æ¬¡è¾“å‡º5ä¸ªå­—ç¬¦ï¼Œå¹³è¡¡é€Ÿåº¦å’Œä½“éªŒ
                        for i in range(0, len(line), chunk_size):
                            chunk = line[i:i+chunk_size]
                            yield generate_sse_chunk(chunk, req_id, current_ai_studio_model_id or MODEL_NAME)
                            await asyncio.sleep(0.03)  # é€‚ä¸­çš„è¾“å‡ºé€Ÿåº¦

                    # æ·»åŠ æ¢è¡Œç¬¦ï¼ˆé™¤äº†æœ€åä¸€è¡Œï¼‰
                    if line_idx < len(lines) - 1:
                        yield generate_sse_chunk('\n', req_id, current_ai_studio_model_id or MODEL_NAME)
                        await asyncio.sleep(0.01)
                
                # è®¡ç®—å¹¶å‘é€å¸¦usageçš„å®Œæˆå—
                usage_stats = calculate_usage_stats(
                    [msg.model_dump() for msg in request.messages],
                    final_content,
                    ""  # Playwrightæ¨¡å¼æ²¡æœ‰reasoning content
                )
                logger.info(f"[{req_id}] Playwrightéæµå¼è®¡ç®—çš„tokenä½¿ç”¨ç»Ÿè®¡: {usage_stats}")
                
                # å‘é€å¸¦usageçš„å®Œæˆå—
                yield generate_sse_stop_chunk(req_id, current_ai_studio_model_id or MODEL_NAME, "stop", usage_stats)
                
            except ClientDisconnectedError as disconnect_err:
                # ä½¿ç”¨æ–°çš„åœæ­¢ä¿¡å·æ£€æµ‹å™¨åˆ†ææ–­å¼€åŸå› 
                abort_handler = AbortSignalHandler()
                disconnect_info = abort_handler.handle_error(disconnect_err, req_id)
                
                logger.info(f"[{req_id}] Playwrightæµå¼ç”Ÿæˆå™¨ä¸­æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€è¿æ¥")
                logger.info(f"[{req_id}] åœæ­¢åŸå› åˆ†æ: {disconnect_info}")
                
                # å®¢æˆ·ç«¯æ–­å¼€æ—¶ç«‹å³è®¾ç½®doneä¿¡å·
                if data_receiving and not completion_event.is_set():
                    logger.info(f"[{req_id}] Playwrightå®¢æˆ·ç«¯æ–­å¼€å¼‚å¸¸å¤„ç†ä¸­ç«‹å³è®¾ç½®doneä¿¡å·")
                    completion_event.set()
            except Exception as e:
                # ä½¿ç”¨æ–°çš„åœæ­¢ä¿¡å·æ£€æµ‹å™¨åˆ†æé”™è¯¯ç±»å‹
                abort_handler = AbortSignalHandler()
                error_info = abort_handler.handle_error(e, req_id)
                
                if error_info['stop_reason'] in ['user_abort', 'client_disconnect']:
                    logger.info(f"[{req_id}] Playwrightæ£€æµ‹åˆ°åœæ­¢ä¿¡å·: {error_info}")
                    # å¯¹äºç”¨æˆ·ä¸»åŠ¨åœæ­¢ï¼Œè§†ä¸ºæ­£å¸¸æš‚åœ
                    if data_receiving and not completion_event.is_set():
                        completion_event.set()
                else:
                    logger.error(f"[{req_id}] Playwrightæµå¼ç”Ÿæˆå™¨å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
                # å‘é€é”™è¯¯ä¿¡æ¯ç»™å®¢æˆ·ç«¯
                try:
                    yield generate_sse_chunk(f"\n\n[é”™è¯¯: {str(e)}]", req_id, current_ai_studio_model_id or MODEL_NAME)
                    yield generate_sse_stop_chunk(req_id, current_ai_studio_model_id or MODEL_NAME)
                except Exception:
                    pass  # å¦‚æœæ— æ³•å‘é€é”™è¯¯ä¿¡æ¯ï¼Œç»§ç»­å¤„ç†ç»“æŸé€»è¾‘
            finally:
                # ç¡®ä¿äº‹ä»¶è¢«è®¾ç½®
                if not completion_event.is_set():
                    completion_event.set()
                    logger.info(f"[{req_id}] Playwrightæµå¼ç”Ÿæˆå™¨å®Œæˆäº‹ä»¶å·²è®¾ç½®")

        stream_gen_func = create_response_stream_generator()
        if not result_future.done():
            result_future.set_result(StreamingResponse(stream_gen_func, media_type="text/event-stream"))
        
        return completion_event, submit_button_locator, check_client_disconnected
    else:
        # ä½¿ç”¨PageControllerè·å–å“åº”
        page_controller = PageController(page, logger, req_id)
        final_content = await page_controller.get_response(check_client_disconnected)
        
        # è®¡ç®—tokenä½¿ç”¨ç»Ÿè®¡
        usage_stats = calculate_usage_stats(
            [msg.model_dump() for msg in request.messages],
            final_content,
            ""  # Playwrightæ¨¡å¼æ²¡æœ‰reasoning content
        )
        logger.info(f"[{req_id}] Playwrightéæµå¼è®¡ç®—çš„tokenä½¿ç”¨ç»Ÿè®¡: {usage_stats}")
        
        response_payload = {
            "id": f"{CHAT_COMPLETION_ID_PREFIX}{req_id}-{int(time.time())}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": current_ai_studio_model_id or MODEL_NAME,
            "choices": [{
                "index": 0,
                "message": {"role": "assistant", "content": final_content},
                "finish_reason": "stop"
            }],
            "usage": usage_stats
        }
        
        if not result_future.done():
            result_future.set_result(JSONResponse(content=response_payload))
        
        return None


async def _cleanup_request_resources(req_id: str, disconnect_check_task: Optional[asyncio.Task], 
                                   completion_event: Optional[Event], result_future: Future, 
                                   is_streaming: bool) -> None:
    """æ¸…ç†è¯·æ±‚èµ„æº - ä¿®å¤æµå¼å“åº”çš„ç›‘æ§ä»»åŠ¡ç”Ÿå‘½å‘¨æœŸ"""
    from server import logger
    
    if is_streaming:
        # å¯¹äºæµå¼å“åº”ï¼Œä¸è¦ç«‹å³å–æ¶ˆç›‘æ§ä»»åŠ¡
        # ç›‘æ§ä»»åŠ¡åº”è¯¥åœ¨æµå¼ç”Ÿæˆå®Œæˆåè‡ªç„¶ç»“æŸæˆ–åœ¨å¼‚å¸¸æ—¶å–æ¶ˆ
        logger.info(f"[{req_id}] æµå¼å“åº”ï¼šç›‘æ§ä»»åŠ¡å°†åœ¨ç”Ÿæˆå®Œæˆåè‡ªç„¶ç»“æŸ")
        
        # åªæœ‰åœ¨å‡ºç°å¼‚å¸¸æ—¶æ‰å¼ºåˆ¶å–æ¶ˆç›‘æ§ä»»åŠ¡
        if result_future.done() and result_future.exception() is not None:
            logger.warning(f"[{req_id}] æµå¼è¯·æ±‚å‘ç”Ÿå¼‚å¸¸ï¼Œå–æ¶ˆç›‘æ§ä»»åŠ¡")
            if disconnect_check_task and not disconnect_check_task.done():
                disconnect_check_task.cancel()
                try: 
                    await disconnect_check_task
                except asyncio.CancelledError: 
                    pass
                except Exception as task_clean_err: 
                    logger.error(f"[{req_id}] æ¸…ç†å¼‚å¸¸ç›‘æ§ä»»åŠ¡æ—¶å‡ºé”™: {task_clean_err}")
        else:
            # æ­£å¸¸æƒ…å†µä¸‹ï¼Œè®©ç›‘æ§ä»»åŠ¡ç»§ç»­è¿è¡Œ
            logger.info(f"[{req_id}] æ­£å¸¸æµå¼å“åº”ï¼šä¿æŒç›‘æ§ä»»åŠ¡æ´»è·ƒçŠ¶æ€")
    else:
        # éæµå¼å“åº”å¯ä»¥ç«‹å³æ¸…ç†ç›‘æ§ä»»åŠ¡
        if disconnect_check_task and not disconnect_check_task.done():
            logger.info(f"[{req_id}] éæµå¼å“åº”ï¼šå–æ¶ˆç›‘æ§ä»»åŠ¡")
            disconnect_check_task.cancel()
            try: 
                await disconnect_check_task
            except asyncio.CancelledError: 
                pass
            except Exception as task_clean_err: 
                logger.error(f"[{req_id}] æ¸…ç†ä»»åŠ¡æ—¶å‡ºé”™: {task_clean_err}")
    
    logger.info(f"[{req_id}] å¤„ç†å®Œæˆã€‚")
    
    if is_streaming and completion_event and not completion_event.is_set() and (result_future.done() and result_future.exception() is not None):
         logger.warning(f"[{req_id}] æµå¼è¯·æ±‚å¼‚å¸¸ï¼Œç¡®ä¿å®Œæˆäº‹ä»¶å·²è®¾ç½®ã€‚")
         completion_event.set()


async def _process_request_refactored(
    req_id: str,
    request: ChatCompletionRequest,
    http_request: Request,
    result_future: Future
) -> Optional[Tuple[Event, Locator, Callable[[str], bool]]]:
    """æ ¸å¿ƒè¯·æ±‚å¤„ç†å‡½æ•° - é‡æ„ç‰ˆæœ¬"""

    # å°†HTTPè¯·æ±‚å¯¹è±¡ä¿å­˜åˆ°å…¨å±€çŠ¶æ€ï¼Œä¾›æµå¼ç”Ÿæˆå™¨ä½¿ç”¨
    import server
    if not hasattr(server, 'current_http_requests'):
        server.current_http_requests = {}
    server.current_http_requests[req_id] = http_request
    
    # ä¼˜åŒ–ï¼šåœ¨å¼€å§‹ä»»ä½•å¤„ç†å‰ä¸»åŠ¨æ£€æµ‹å®¢æˆ·ç«¯è¿æ¥çŠ¶æ€
    is_connected = await _test_client_connection(req_id, http_request)
    if not is_connected:
        from server import logger
        logger.info(f"[{req_id}]  æ ¸å¿ƒå¤„ç†å‰æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€ï¼Œæå‰é€€å‡ºèŠ‚çœèµ„æº")
        # æ¸…ç†å…¨å±€çŠ¶æ€
        server.current_http_requests.pop(req_id, None)
        if not result_future.done():
            result_future.set_exception(HTTPException(status_code=499, detail=f"[{req_id}] å®¢æˆ·ç«¯åœ¨å¤„ç†å¼€å§‹å‰å·²æ–­å¼€è¿æ¥"))
        return None

    context = await _initialize_request_context(req_id, request)
    context = await _analyze_model_requirements(req_id, context, request)
    
    page = context['page']
    client_disconnected_event, disconnect_check_task, check_client_disconnected = await _setup_disconnect_monitoring(
        req_id, http_request, result_future, page
    )
    
    submit_button_locator = page.locator(SUBMIT_BUTTON_SELECTOR) if page else None
    completion_event = None
    skip_button_monitor_task = None
    
    try:
        await _validate_page_status(req_id, context, check_client_disconnected)
        
        page_controller = PageController(page, context['logger'], req_id)

        await _handle_model_switching(req_id, context, check_client_disconnected)
        await _handle_parameter_cache(req_id, context)
        
        system_prompt, prepared_prompt, image_list = await _prepare_and_validate_request(req_id, request, check_client_disconnected)

        # åœ¨è°ƒæ•´å…¶ä»–å‚æ•°ä¹‹å‰è®¾ç½®ç³»ç»ŸæŒ‡ä»¤
        await page_controller.set_system_instructions(system_prompt, check_client_disconnected)

        # ä½¿ç”¨PageControllerå¤„ç†é¡µé¢äº¤äº’
        # æ³¨æ„ï¼šèŠå¤©å†å²æ¸…ç©ºå·²ç§»è‡³é˜Ÿåˆ—å¤„ç†é”é‡Šæ”¾åæ‰§è¡Œ

        await page_controller.adjust_parameters(
            request.model_dump(exclude_none=True), # ä½¿ç”¨ exclude_none=True é¿å…ä¼ é€’Noneå€¼
            context['page_params_cache'],
            context['params_cache_lock'],
            context['model_id_to_use'],
            context['parsed_model_list'],
            check_client_disconnected
        )

        # ä¼˜åŒ–ï¼šåœ¨æäº¤æç¤ºå‰å†æ¬¡æ£€æŸ¥å®¢æˆ·ç«¯è¿æ¥ï¼Œé¿å…ä¸å¿…è¦çš„åå°è¯·æ±‚
        check_client_disconnected("æäº¤æç¤ºå‰æœ€ç»ˆæ£€æŸ¥")

        await page_controller.submit_prompt(prepared_prompt, image_list, check_client_disconnected)
        
        # å¯åŠ¨ "Skip" æŒ‰é’®çš„åå°ç›‘æ§ä»»åŠ¡
        skip_button_stop_event = asyncio.Event()
        skip_button_monitor_task = asyncio.create_task(
            page_controller.continuously_handle_skip_button(skip_button_stop_event, check_client_disconnected)
        )

        # å“åº”å¤„ç†ä»ç„¶éœ€è¦åœ¨è¿™é‡Œï¼Œå› ä¸ºå®ƒå†³å®šäº†æ˜¯æµå¼è¿˜æ˜¯éæµå¼ï¼Œå¹¶è®¾ç½®future
        response_result = await _handle_response_processing(
            req_id, request, page, context, result_future, submit_button_locator, check_client_disconnected, disconnect_check_task
        )
        
        if response_result:
            completion_event, _, _ = response_result
        
        return completion_event, submit_button_locator, check_client_disconnected
        
    except ClientDisconnectedError as disco_err:
        context['logger'].info(f"[{req_id}] æ•è·åˆ°å®¢æˆ·ç«¯æ–­å¼€è¿æ¥ä¿¡å·: {disco_err}")
        if not result_future.done():
             result_future.set_exception(HTTPException(status_code=499, detail=f"[{req_id}] Client disconnected during processing."))
    except HTTPException as http_err:
        context['logger'].warning(f"[{req_id}] æ•è·åˆ° HTTP å¼‚å¸¸: {http_err.status_code} - {http_err.detail}")
        if not result_future.done():
            result_future.set_exception(http_err)
    except PlaywrightAsyncError as pw_err:
        context['logger'].error(f"[{req_id}] æ•è·åˆ° Playwright é”™è¯¯: {pw_err}")
        await save_error_snapshot(f"process_playwright_error_{req_id}")
        if not result_future.done():
            result_future.set_exception(HTTPException(status_code=502, detail=f"[{req_id}] Playwright interaction failed: {pw_err}"))
    except Exception as e:
        context['logger'].exception(f"[{req_id}] æ•è·åˆ°æ„å¤–é”™è¯¯")
        await save_error_snapshot(f"process_unexpected_error_{req_id}")
        if not result_future.done():
            result_future.set_exception(HTTPException(status_code=500, detail=f"[{req_id}] Unexpected server error: {e}"))
    finally:
        # åœæ­¢ "Skip" æŒ‰é’®ç›‘æ§ä»»åŠ¡
        if 'skip_button_stop_event' in locals() and skip_button_stop_event:
            skip_button_stop_event.set()
        if skip_button_monitor_task:
            try:
                await asyncio.wait_for(skip_button_monitor_task, timeout=2.0)
            except asyncio.TimeoutError:
                context['logger'].warning(f"[{req_id}] 'Skip' æŒ‰é’®ç›‘æ§ä»»åŠ¡å…³é—­è¶…æ—¶ã€‚")
            except Exception as e:
                context['logger'].error(f"[{req_id}] 'Skip' æŒ‰é’®ç›‘æ§ä»»åŠ¡æ¸…ç†æ—¶å‘ç”Ÿé”™è¯¯: {e}")

        # ä»è¯·æ±‚ç®¡ç†å™¨ä¸­æ³¨é”€è¯·æ±‚
        request_manager.unregister_request(req_id)
        
        # å…¨å±€HTTPè¯·æ±‚çŠ¶æ€å°†åœ¨æµå¼ç”Ÿæˆå™¨ç»“æŸæ—¶æ¸…ç†ï¼Œæ­¤å¤„ä¸å†å¤„ç†
        
        await _cleanup_request_resources(req_id, disconnect_check_task, completion_event, result_future, request.stream)